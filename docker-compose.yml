name: minerva
services:
  # NGINX as central reverse proxy
  nginx:
    image: nginx:alpine
    container_name: nginx-proxy
    networks:
      - minerva-network
    ports:
      - "127.0.0.1:80:80"
    depends_on:
      ollama:
        condition: service_healthy
    volumes:
      - "./docker/nginx/conf/nginx.conf:/etc/nginx/nginx.conf"
      - "./docker/nginx/ssl:/etc/nginx/ssl"  # Direct mount from host
      - ollama-webui-data:/usr/local/lib/python3.11/dist-packages/open_webui/data
    restart: unless-stopped
    environment:
      - NGINX_PROXY_WEBSOCKET_TIMEOUT=60s
      - NGINX_CLIENT_MAX_BODY_SIZE=100M
    logging:
      options:
        max-size: "50m"
        max-file: "3"
        compress: "true"    

  # Tailscale for secure remote access
  tailscale:
    image: tailscale/tailscale:latest
    container_name: tailscale
    hostname: minerva
    networks:
      - minerva-network
    volumes:
      - tailscale-data:/var/lib/tailscale
      - "./docker/startup/tailscale.sh:/tailscale.sh"
      - "./docker/nginx/ssl:/etc/nginx/ssl"  # Change back to named volume
    cap_add:
      - NET_ADMIN
      - NET_RAW
    environment:
      - TS_AUTHKEY=${TAILSCALE_AUTH_KEY}
      - TS_HOSTNAME=minerva
    restart: unless-stopped
    entrypoint: ["/bin/sh", "/tailscale.sh"]  # Simplify entrypoint
    depends_on:
      - nginx
    env_file:
      - .env
    logging:      
      options:
        max-size: "50m"
        max-file: "3"
        compress: "true"

  # Ollama Service with WebUI, API, Jupyter and Tika
  ollama:
    image: ollama-minerva:custom
    container_name: ollama-container
    build:
      context: ./docker/ollama
      dockerfile: ollama.dockerfile       
    networks:
      - minerva-network
    ports:
      - "127.0.0.1:11434:11434"
      - "127.0.0.1:8080:8080" 
    restart: unless-stopped
    volumes:
      - "./docker/ollama/config/config.json:/root/.ollama/config.json"
      - "./docker/ollama/custom/assets:/app/custom/assets"
      - ollama-data:/root/.ollama/models
      - "./docker/startup/ollama.sh:/app/ollama.sh"
      - "./docker/huggingface_cache:/root/.cache/huggingface"
    entrypoint: ["/bin/bash", "-c", "chmod +x /app/ollama.sh && /app/ollama.sh"]    
    environment:
      #############################################
      # SYSTEM & BASE URL CONFIGURATION
      #############################################
      - NVIDIA_VISIBLE_DEVICES=0
      - OLLAMA_BASE_URL=http://ollama-container:11434
      - RAG_OLLAMA_BASE_URL=http://ollama-container:11434
      - SENTENCE_TRANSFORMERS_HOME=/root/.cache/huggingface/sentence_transformers
      - TIKA_SERVER_URL=http://tika-server:9998
      - OPENAI_API_BASE_URLS=https://api.groq.com/openai/v1;https://openrouter.ai/api/v1
      #############################################
      # API KEYS
      #############################################
      - OLLAMA_API_KEY=${OLLAMA_API_KEY} 
      - WEBUI_OLLAMA_API_KEY=${OLLAMA_API_KEY}      
      - OPENAI_API_KEYS=${GROQ_API_KEY};${OPENROUTER_API_KEY}                 
      - HUGGINGFACE_HUB_TOKEN=${HUGGINGFACE_HUB_TOKEN}      
      - GOOGLE_PSE_API_KEY=${GOOGLE_API_KEY}
      - GOOGLE_PSE_ENGINE_ID=${GOOGLE_CSE_ID}            
      - WEBUI_JUPYTER_TOKEN=${JUPYTER_TOKEN}
      - FINNHUB_API_KEY=${FINNHUB_API_KEY}
      #############################################
      # PIPELINES CONFIGURATION
      #############################################
      - WEBUI_PIPELINES_ENABLED=true
      - WEBUI_PIPELINES_URL=http://pipelines:9099
      # WEBUI_PIPELINES_API_KEY=${OLLAMA_API_KEY}  # Use the same API key      
      #############################################
      # OLLAMA CORE CONFIGURATION
      #############################################
      - OLLAMA_GPU_LAYERS=-1
      - OLLAMA_KEEP_ALIVE=24h
      - DEFAULT_LOCALE=en-US 
      - WEBUI_LANGUAGE=en-US                   
      - WEBUI_CHAT_LANGUAGE=en-US
      - ENABLE_EVALUATION_ARENA_MODELS=false
      - OLLAMA_PRELOAD=qwen2.5:14b
      # WEBUI_CHAT_TEMPLATE=qwen2    
      - WEBUI_CHAT_COMPLETION_PARSE_MODE=auto
      - RAG_EMBEDDING_ENGINE=ollama 
      - RAG_EMBEDDING_MODEL=nomic-embed-text          
      - RAG_RERANKING_MODEL=mixedbread-ai/mxbai-rerank-base-v1
      # WEBUI_VISION_DETECTIONS=true                  # Non-default (default is false)
      # WEBUI_DEBUG_TOOLS=false                       # Non-default (default is false)
      #############################################
      # AUDIO CONFIGURATION
      #############################################      
      - WEBUI_ENABLE_TRANSCRIPTION=true
      # AUDIO_STT_ENGINE=openai
      # AUDIO_STT_OPENAI_API_BASE_URL=https://api.groq.com/openai/v1
      # AUDIO_STT_OPENAI_API_KEY=${GROQ_API_KEY}
      - WHISPER_MODEL=large-v3-turbo
      - WEBUI_TRANSCRIPTION_LANGUAGE=auto
      - WEBUI_TRANSCRIPTION_DEVICE=cuda
      - WEBUI_TRANSCRIPTION_COMPUTE_TYPE=float16      
      - AUDIO_TTS_ENGINE=openai
      - AUDIO_TTS_OPENAI_API_BASE_URL=http://kokoro-tts:8880/v1      
      - AUDIO_TTS_OPENAI_API_KEY=${KOKORO_API_KEY}
      #############################################
      # AUTOMATIC1111 CONFIGURATION
      #############################################    
      - AUTOMATIC1111_SAMPLER=DPM++ 2M SDE
      - AUTOMATIC1111_SCHEDULER=Karras  
      - AUTOMATIC1111_CFG_SCALE=6.5
      - ENABLE_IMAGE_GENERATION=true                           # Enable image generation globally
      - WEBUI_ENABLE_IMAGE_GENERATION=true
      - IMAGE_GENERATION_MODEL=dreamshaper_xl_v2.0.safetensors
      - IMAGE_SIZE=1024x1024
      - IMAGE_STEPS=50
      - IMAGE_GENERATION_ENGINE=automatic1111                    # Set provider to AUTOMATIC1111
      - WEBUI_IMAGE_PROVIDER=automatic1111
      - AUTOMATIC1111_BASE_URL=http://stable-diffusion:7860   # Point to SD container
      - AUTOMATIC1111_API_AUTH=${SD_WEBUI_AUTH}
      #############################################
      # Jupyter configuration
      #############################################    
      - ENABLE_CODE_INTERPRETER=true
      - CODE_EXECUTION_ENGINE=jupyter
      - CODE_EXECUTION_JUPYTER_URL=http://jupyter-notebook:8888
      - CODE_INTERPRETER_ENGINE=jupyter
      - CODE_INTERPRETER_JUPYTER_URL=http://jupyter-notebook:8888
      - CODE_EXECUTION_JUPYTER_AUTH= 
      - CODE_INTERPRETER_JUPYTER_AUTH=
      #############################################
      # ADMIN PANEL CONFIGURATION
      #############################################      
      - CHUNK_SIZE=1440                               # Admin Panel: Chunk Size
      - CHUNK_OVERLAP=144                             # Admin Panel: Chunk Overlap
      - RAG_EMBEDDING_BATCH_SIZE=32                   # Admin Panel: Embedding Batch Size (add this)
      - RAG_TOP_K=3                                   # Admin Panel: Top K
      - RAG_CITATION_SOURCE_LIMIT=3                   # Limit citations to top sources only  
      - ENABLE_RAG_WEB_SEARCH=true                    # Enable web search globally
      - ENABLE_SEARCH_QUERY_GENERATION=true           # Enable search query generation globally                
      - RAG_WEB_SEARCH_ENGINE=google_pse
      - ENABLE_GOOGLE_DRIVE_INTEGRATION=true
      - GOOGLE_DRIVE_API_KEY=${GOOGLE_API_KEY}
      - GOOGLE_DRIVE_CLIENT_ID=${GOOGLE_CLIENT_ID}
      # RAG_WEB_SEARCH_TRUST_ENV=true  
      # ENABLE_RAG_WEB_LOADER_SSL_VERIFICATION=true 
      - RAG_SEARCH_TERM_EXTRACTION_COUNT=2            # Reduce number of generated search terms 
      - RAG_WEB_SEARCH_RESULT_COUNT=2                 # Admin Panel: Search Result Count
      - RAG_WEB_SEARCH_CONCURRENT_REQUESTS=1          # Admin Panel: Concurrent Web Search Queries
      - RAG_MAX_CONSECUTIVE_SEARCHES=1                # Limit consecutive searches
      - WEBUI_WEB_SEARCH_TIMEOUT=10                   # Search timeout in seconds                
      - RAG_MINIMUM_SCORE=0.77                        
      - RAG_CITATION_MIN_RELEVANCE=0.77               # Only cite highly relevant sources         
      - RAG_CONTENT_FILTER_THRESHOLD=0.8  
      - PDF_EXTRACT_IMAGES=true                       # Extract images from PDFs     
      - RAG_CONTENT_COMPRESSION=true                  # Enable content compression lower context loss
      - RAG_REDUNDANCY_FILTER=high                    # Remove redundant content more aggressively lower context loss 
      #############################################
      # TOKEN SIZE CONFIGURATION
      #############################################
      - OLLAMA_CONTEXT_LENGTH=8192
      - OLLAMA_MAX_NEW_TOKENS=2048
      - RAG_MAX_TOTAL_TOKENS=6800    
      - RAG_MAX_INPUT_TOKENS=5800                     # Limit initial input size before processing
      #############################################
      # REDIS
      #############################################
      - ENABLE_WEBSOCKET_SUPPORT=true
      - WEBSOCKET_MANAGER=redis
      - WEBSOCKET_REDIS_URL=redis://redis:6379/0
      #############################################
      # QUERY CONTROLS
      #############################################
      # RAG_QUERY_GENERATION_MODE=simple
      # RAG_FORCE_SMALL_CHUNKS=true             
      # RAG_PRIORITIZE_KEYWORDS=true            
      # RAG_CONTENT_TYPE_BOOST=news:1.5         
      # RAG_EXTRACT_TITLE_BOOST=1.2                   # Boost relevance of content with matching titles 
      # WEBUI_SEARCH_QUERY_MAPPING=different          # Prevent auto-chunking questions
      # RAG_WEB_SEARCH_REUSE=false                    # Don't accumulate search results
      # RAG_CLEAR_HISTORY_BETWEEN_SEARCHES=true       # Reset history between searches   
      # RAG_FORCE_CONTEXT_RESET_AFTER_SEARCHES=true   # Reset context after multiple searches
      # WEBUI_CONVERSATION_MEMORY_WINDOW=4            # Keep only recent messages in memory  
      #############################################
      # IMAGE SEARCH CONFIGURATION
      #############################################
      # RAG_INCLUDE_IMAGES=true
      # RAG_IMAGE_MAX_SIZE=1024
      # RAG_WEB_SEARCH_INCLUDE_IMAGES=true
      # RAG_IMAGE_HANDLING=base64
      # RAG_IMAGE_PROCESSOR=auto
      # GOOGLE_PSE_SEARCH_TYPE=image,standard
      # GOOGLE_PSE_IMAGE_SIZE=large    
      #############################################
      # PROMPT CONFIGURATION
      #############################################      
      # RAG_PROMPT_TEMPLATE="<search_results>\n{results}\n</search_results>\n\nUse ONLY the information in the search results to answer."
      # RAG_SYSTEM_PREFIX="Your name is minerva. You are an AI assistant with access to search results and attached documents. Use attached documents when available, otherwise use search results provided."
      # RAG_REFERENCE_FORMAT="[{source}]({url})"
      # RAG_IMAGES_TEMPLATE="<search_images>\n{images}\n</search_images>"
      # RAG_RESULT_TEMPLATE="<search_results>\n{results}\n{images}\n</search_results>"    
      # RAG_TEXT_BEFORE_PROMPT="Here are search results from the web:"
      # RAG_TEXT_AFTER_PROMPT="You must ONLY use the information from these search results to answer. If the search results don't provide enough information, say so."
      #############################################
      # RAG CORE CONFIGURATION
      #############################################
      # RAG_MODE=direct_search                        # default is "rag",others are "hybrid","web_search","direct_search"
      # RAG_SYSTEM_PROMPT_STRATEGY=explicit_search      
      # RAG_ACKNOWLEDGE_TRUNCATION=true               # Force model to acknowledge when content is truncated
      # RAG_PROMPT_STRATEGY=trim                      # Default for Google search           
      # RAG_FORCE_CITATIONS=true                 
      # RAG_CITATION_FORMAT=full_source
      # RAG_VECTOR_DB_CLEAN_START=true                # default is false
      # RAG_SMART_TRUNCATION=true                     # Use smart truncation to keep most relevant content
      - RAG_TRUNCATION_STRATEGY=relevance             # Prioritize most relevant chunks when truncating
      # WEBUI_RAG_PRIORITIZE_RECENT=true              # Prioritize recent content
      #############################################
      # RAG CACHING
      #############################################
      # WEBUI_CLEAR_CONTEXT_BETWEEN_TOOLS=true        # Critical - clears context when switching tools
      # RAG_CLEAR_CACHE_ON_TOOL_SWITCH=true           # Clears vector cache when switching tools
      # RAG_VECTOR_DB_COLLECTION_PER_QUERY=true       # Isolate search results between queries
      # RAG_CLEAR_FILE_CONTEXT=true                   # Would clear file contexts (not in current OpenWebUI)
      # RAG_VECTOR_DB_SESSION_ISOLATION=true          # Isolates vector collections between operations
      # RAG_FORCE_CONTEXT_INCLUSION=true
      # RAG_PREPEND_QUERY_TO_RETRIEVAL=true           # Keep content related to query
      # RAG_CACHE_RESULTS=true
    env_file:
      - .env
    logging:      
      options:
        max-size: "50m"
        max-file: "3"
        compress: "true"      
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, utility, video]
    healthcheck:
      test: ["CMD", "bash", "-c", "curl -f http://localhost:8080/health && curl -f http://localhost:11434/api/version"]
      interval: 30s
      timeout: 15s
      retries: 3
      start_period: 60s

  jupyter-notebook:
    image: jupyter-minerva:custom
    container_name: jupyter-notebook
    build:
      context: ./docker/jupyter
      dockerfile: jupyter.dockerfile    
    networks:
      - minerva-network
    ports:
      - "127.0.0.1:8888:8888"
    restart: unless-stopped
    volumes:
      - jupyter-data:/home/jovyan/work
    environment:
    - JUPYTER_DISABLE_CHECK_XSRF=1
    - JUPYTER_ENABLE_LAB=1
    - JUPYTER_TOKEN=${JUPYTER_TOKEN}
    - CORS_ALLOW_ORIGIN=http://localhost
    - JUPYTER_ALLOW_ORIGIN=http://localhost
    command: >
      jupyter lab
      --NotebookApp.base_url=/jupyter/
      --NotebookApp.allow_remote_access=true
      --NotebookApp.token=${JUPYTER_TOKEN}
      --NotebookApp.allow_origin=http://localhost
    logging:      
      options:
        max-size: "50m"
        max-file: "3"
        compress: "true"    
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8888/jupyter/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  tika-server:
    image: apache/tika:latest-full
    container_name: tika-server
    networks:
      - minerva-network
    restart: unless-stopped
    ports:
      - "127.0.0.1:9998:9998"
    volumes:
      - "tika-data:/usr/local/tika"
    logging:      
      options:
        max-size: "50m"
        max-file: "3"
        compress: "true"      

  # Stable Diffusion Service
  stable-diffusion:
    image: stable-diffusion-minerva:custom
    container_name: stable-diffusion
    build:
      context: ./docker/stable-diffusion
      dockerfile: stable-diffusion.dockerfile   
    networks:
      - minerva-network
    ports:
      - "127.0.0.1:7860:7860" # Expose the FastAPI port
    restart: unless-stopped
    volumes:
      - stable-diffusion-models:/app/stable-diffusion-webui/models
      - stable-diffusion-outputs:/app/stable-diffusion-webui/outputs
      - stable-diffusion-repositories:/app/stable-diffusion-webui/repositories
      - stable-diffusion-venv:/app/stable-diffusion-webui/venv
      - stable-diffusion-extensions:/app/stable-diffusion-webui/extensions
      - ./docker/startup/stable-diffusion.sh:/tmp/stable-diffusion.sh  # Mount to temp location
    depends_on:
      - ollama
    entrypoint: ["/bin/bash", "-c", "cp /tmp/stable-diffusion.sh /app/start.sh && chmod +x /app/start.sh && /app/start.sh"]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - SD_ATTENTION_SLICING=true
      - SD_TURBO_MODE=false
      - SD_MAX_BATCH_SIZE=1
      - SD_WEBUI_AUTH=${SD_WEBUI_AUTH}  # Pass through for our script
    logging:      
      options:
        max-size: "50m"
        max-file: "3"
        compress: "true"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, utility, video]

  # SearXNG Service (Enable this if preferred over Google PSE)
#  searxng:
#    image: searxng-custom:1.0.5
#    container_name: searxng
#    networks:
#      - minerva-network
#    ports:
#      - "127.0.0.1:8080:8080"      
#    dns:
#      - 8.8.8.8
#      - 1.1.1.1      
#    restart: unless-stopped
#    volumes:
#      # Keep your custom settings file
#      - "./docker/searxng/config/settings.yml:/etc/searxng/settings.yml"
#      - searxng-data:/etc/searxng
#    environment:
#      # Only set critical environment variables, let settings.yml handle the rest
#      - SEARXNG_BASE_URL=http://searxng:8080/
#      - SEARXNG_REDIS_URL=redis://redis:6379/0
#      - SEARXNG_SECRET=${SEARXNG_SECRET_KEY}
#    logging:#      
#      options:
#        max-size: "50m"
#        max-file: "3"
#        compress: "true"
#    healthcheck:
#      test: ["CMD", "wget", "-q", "--spider", "http://localhost:8080/healthz"]
#      interval: 30s
#      timeout: 10s
#      retries: 3

  # Replace with correct Kokoro-FastAPI image
  kokoro-tts:
    image: ghcr.io/remsky/kokoro-fastapi-gpu:v0.2.2  # Correct image path
    container_name: kokoro-tts
    networks:
      - minerva-network
    restart: unless-stopped
    ports:
      - "127.0.0.1:8880:8880"  
    volumes:
      - kokoro-models:/app/models
      - kokoro-speakers:/app/speakers
    depends_on:
      - ollama
    # GPU configuration
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              count: 1
              capabilities: [gpu, compute, utility]
    environment:
      - NVIDIA_VISIBLE_DEVICES=0
      - MODEL_ID=eroglas/XTTS-v2
      - ENABLE_MPS=true
    logging:      
      options:
        max-size: "50m"
        max-file: "3"
        compress: "true"

  # Add this service
  redis:
    image: redis:alpine
    container_name: redis 
    restart: always
    networks:
      - minerva-network
    volumes:
      - redis-data:/data
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
    command: redis-server --save 60 1 --loglevel warning
    logging:      
      options:
        max-size: "20m"
        max-file: "2"
        compress: "true"      

  # Add to services section
  pipelines:
    image: pipelines-minerva:custom
    container_name: pipelines
    build:
      context: ./docker/pipelines
      dockerfile: pipelines.dockerfile
    networks:
      - minerva-network
    restart: unless-stopped
    ports:
      - "127.0.0.1:9099:9099"
    volumes:
      - pipelines-data:/app/data
    environment:
      - PIPELINES_SERVER_PORT=9099
      - PIPELINES_SERVER_HOST=0.0.0.0
      - OLLAMA_API_BASE_URL=http://ollama-container:11434
    logging:
      options:
        max-size: "50m"
        max-file: "3"
        compress: "true"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9099"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s
      
  langchain:
    image: langchain-minerva:custom
    container_name: langchain-container
    build:
      context: ./docker/langchain
      dockerfile: langchain.dockerfile
    networks:
      - minerva-network
    ports:
      - "127.0.0.1:5000:5000"
    restart: unless-stopped
    volumes:
      - "./docker/langchain:/app"
    environment:
      - OLLAMA_API_BASE_URL=http://ollama-container:11434
    logging:
      options:
        max-size: "50m"
        max-file: "3"
        compress: "true"  

# Networks
networks:
  minerva-network:
    name: minerva-network
    driver: bridge
    # Add DNS configuration to ensure internet access
    ipam:
      config:
        - subnet: 172.21.0.0/16

# Volumes
volumes:
  tailscale-data:
  ollama-data:
  pipelines-data:
#  searxng-data:
  nginx-ssl-data:
  jupyter-data:
  tika-data:
  stable-diffusion-models:
  stable-diffusion-outputs:
  stable-diffusion-repositories:   
  stable-diffusion-venv:     
  stable-diffusion-extensions:    
  kokoro-models:
  kokoro-speakers:
  redis-data:
  ollama-webui-data: